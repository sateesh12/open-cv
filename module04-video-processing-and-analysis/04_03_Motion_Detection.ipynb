{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZtNd7YoUeq5B"
   },
   "source": [
    "<h1 style=\"font-size:30px;\">Motion Detection in Videos</h1> \n",
    "\n",
    "In this notebook, we are going to demonstrate how to detect motion in videos that contain moving foreground objects. Topics covered in this notebook include:\n",
    "\n",
    "* Using a background subtraction model to isolate moving objects in the foreground (creating a foreground mask)\n",
    "* Using a method called \"erosion\" to remove noise from the foreground mask.\n",
    "\n",
    "Background subtraction models work by building a statistical model of the background scene based on previous frames in a video stream. The model can then be used to create a foreground mask of the current frame by comparing the current frame with the model of the background. If the current frame is of the \"same\" background scene then the foreground mask should theoretically be entirely black. If anything changes in the scene then the foreground mask will detect the changes and record them as non-zero pixel intensities where the intensity in the foreground mask is a measure of the change in the scene. The foreground mask can therefore be used to identify motion in a video stream. In this notebook we will identify the motion by drawing a bounding rectangle that encompasses all the non-zero pixels in the foreground mask.\n",
    "\n",
    "Erosion is a morphological operation that has many uses in computer vision. One use of erosion is to remove small amounts of noise in binary images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Vsa2P71eq5E"
   },
   "source": [
    "# 1. Preview Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iubvIYbheq5F"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print(\"Downloading Code to Colab Environment\")\n",
    "    !wget https://www.dropbox.com/sh/t7x50ww3ultvwn3/AACnmQGBD7rIbznXzi1sRWJqa?dl=1 -O module-code.zip -q --show-progress\n",
    "    !unzip -qq module-code.zip\n",
    "    !pip install moviepy==0.2.3.5\n",
    "    !pip install imageio==2.4.1\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlT1GgRleq5G",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "\n",
    "input_video = 'motion_test.mp4'\n",
    "\n",
    "# Load the video for playback. \n",
    "clip = VideoFileClip(input_video)\n",
    "clip.ipython_display(width = 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t80ss77ceq5G"
   },
   "source": [
    "# 2. Work Flow\n",
    "\n",
    "* Create a statistical model of the background scene using **`createBackgroundSubtractorKNN()`**\n",
    "* For each video frame:\n",
    "    * Compare the current frame with the background model to create a foreground mask using the **`apply()`** method\n",
    "    * Apply erosion to the foreground mask to reduce noise using **`erode()`**\n",
    "    * Find the bounding rectangle that encompasses all the non-zero points in the foreground mask using  **`boundingRect()`**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yjCGBIWpeq5H"
   },
   "source": [
    "## 2.1 Workflow Preview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JQZ4-nT6eq5H"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Static scene (no motion in the video)</font>\n",
    "The top left frame (Foreground Mask) is almost entirely black with the exception of a few pixels that have non-zero intensity. Even though nothing apparent is changing in the scene from frame to frame, adjacent frames in a video stream may have very subtle differences due to the way the camera sensor is perceiving the scene, or the scene itself could be changing ever so slightly from lighting variations (reflections, shadows, etc.). The red bounding rectangle shown to the right encompasses all the non-zero pixels in the foreground mask. Note that some non-zero pixels are not even visible since a single pixel is extremely small in this high-resolution image.\n",
    "\n",
    "The bottom left frame (Foreground Mask Eroded) shows the effect of applying erosion to the Foreground Mask (eliminating the noise entirely)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4iV18EQ4eq5I"
   },
   "source": [
    "![Still-2-secc](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Still-2sec-03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7kF6KRyeq5J"
   },
   "source": [
    "The examples below illustrate how erosion works. We begin by defining a small structuring element called a kernel. In this example, we are using a 3x3 kernel. The center of the kernel (yellow dot) is placed over every pixel ***p*** in the original image. At each pixel location, ***p***, we then examine the surrounding pixels beneath the kernel. If ALL the pixels beneath the kernel are white then the pixel ***p*** in the eroded image remains white.  Otherwise, the pixel in the eroded image is set to black. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9M3OmMY7eq5J"
   },
   "source": [
    "![Erosion](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Erosion-03.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xAXzn2Txeq5K"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Dynamic scene (obvious motion in the video)</font>\n",
    "The top left frame (Foreground Mask) is detecting large changes in the scene due to the motion of the book, but the foreground mask also contains a lot of background noise from subtle lighting changes (e.g., shadows).\n",
    "\n",
    "The bottom left frame (Foreground Mask Eroded) shows the effect of applying erosion to the Foreground Mask. Most of the noise associated with shadows has been suppressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFnzFiVleq5K"
   },
   "source": [
    "![Feature-Image-03a](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Feature-Image-03a.jpg)\n",
    "![Feature-Image-03b](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Feature-Image-03b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jFvS0-1feq5K"
   },
   "source": [
    "# 3. Documentation Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "701xL_0meq5L"
   },
   "source": [
    "## 3.1 KNN Background Subtractor\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "**`createBackgroundSubtractorKNN()`** creates KNN Background Subtractor.\n",
    "\n",
    "### <font color=\"green\">Function Syntax </font>\n",
    "``` python\n",
    "retval = cv2.createBackgroundSubtractorKNN([, history[, dist2Threshold[, detectShadows]]])\t\n",
    "```\n",
    "\n",
    "`retval`: KNN Background Subtractor object.\n",
    "\n",
    "The function has 3 optional arguments:\n",
    "\n",
    "1. `history` Length of the history.\n",
    "2. `dist2Threshold` Threshold on the squared distance between the pixel and the sample to decide whether a pixel is close to that sample. This parameter does not affect the background update.\n",
    "3. `detectShadows` If true, the algorithm will detect shadows and mark them. It decreases the speed a bit, so if you do not need this feature, set the parameter to false.\n",
    "\n",
    "\n",
    "### <font color=\"green\">OpenCV Documentation</font>\n",
    "\n",
    "[**`createBackgroundSubtractorKNN()`**](https://docs.opencv.org/4.5.2/de/de1/group__video__motion.html#gac9be925771f805b6fdb614ec2292006d)\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XcKsgkDkeq5L"
   },
   "source": [
    "## 3.2 Applying Background Subtractor\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "**`apply()`** computes a foreground mask.\n",
    "\n",
    "### <font color=\"green\">Function Syntax </font>\n",
    "``` python\n",
    "fgmask = cv2.BackgroundSubtractor.apply(image[, fgmask[, learningRate]])\t\n",
    "```\n",
    "\n",
    "The function has **1 required input argument** and 2 optional flags:\n",
    "\n",
    "1. `image` Next video frame.\n",
    "2. `fgmask` The output foreground mask as an 8-bit binary image.\n",
    "3. `learningRate` The value between 0 and 1 that indicates how fast the background model has learned. Negative parameter value makes the algorithm to use some automatically chosen learning rate. 0 means that the background model is not updated at all, 1 means that the background model is completely reinitialized from the last frame.\n",
    "\n",
    "### <font color=\"green\">OpenCV Documentation</font>\n",
    "\n",
    "[**`apply()`**](https://docs.opencv.org/4.5.2/d7/df6/classcv_1_1BackgroundSubtractor.html#aa735e76f7069b3fa9c3f32395f9ccd21)<br>\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O8iFaHueeq5M"
   },
   "source": [
    "## 3.3 Erosion\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "**`erode()`** erodes an image by using a specific structuring element.\n",
    "\n",
    "### <font style = \"color:rgb(8,133,37)\">Function Syntax </font>\n",
    "``` python\n",
    "dst = cv2.erode(src, kernel[, dst[, anchor[, iterations[, borderType[, borderValue]]]]])\t\n",
    "```\n",
    "\n",
    "`dst`: output image of the same size and type as src. \n",
    "\n",
    "The function has **2 required input arguments**:\n",
    "\n",
    "1. `src` input image; the number of channels can be arbitrary, but the depth should be one of CV_8U, CV_16U, CV_16S, CV_32F or CV_64F.\n",
    "2. `kernel`\tstructuring element used for erosion; if element = Mat(), a 3 x 3 rectangular structuring element is used. Kernel can be created using getStructuringElement.\n",
    "\n",
    "\n",
    "### <font color=\"green\">OpenCV Documentation</font>\n",
    "\n",
    "[**`erode()`**](https://docs.opencv.org/3.4/d4/d86/group__imgproc__filter.html#gaeb1e0c1033e3f6b891a25d0511362aeb)<br>\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XlyRoB-Feq5M"
   },
   "source": [
    "## 3.4 findNonZero\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "**`findNonZero()`** Returns the list of locations of non-zero pixels.\n",
    "\n",
    "\n",
    "### <font color=\"green\">Function Syntax </font>\n",
    "``` python\n",
    "retval = cv2.findNonZero(src)\t\n",
    "```\n",
    "\n",
    "The function has **1 required input argument**:\n",
    "\n",
    "1. `src` single-channel array.\n",
    "\n",
    "### <font color=\"green\">OpenCV Documentation</font>\n",
    "\n",
    "[**`findNonZero()`**](https://docs.opencv.org/4.5.2/d2/de8/group__core__array.html#gaed7df59a3539b4cc0fe5c9c8d7586190) <br>\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wcubdHP3eq5M"
   },
   "source": [
    "## 3.5 Bounding Rectangle\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />\n",
    "\n",
    "**`boundingRect()`** calculates and returns the minimal up-right bounding rectangle for the specified point set or non-zero pixels of gray-scale image.\n",
    "\n",
    "### <font color=\"green\">Function Syntax </font>\n",
    "``` python\n",
    "retval = cv2.boundingRect(array)\t\n",
    "\n",
    "retval is a tuple that contains four values: (x, y, w, h)\n",
    "    \n",
    "    (x,y) are the coordinates for the upper left coner of the bounding rectatngle\n",
    "    (w,h) are the width and height of the bounding rectangle\n",
    "    \n",
    "The function can also be called to explicitly return the individual values:\n",
    "    \n",
    "x, y, w, h  = cv2.boundingRect(array)  \n",
    "        \n",
    "```\n",
    "\n",
    "The function has **1 required input argument**:\n",
    "\n",
    "1. `array` Input gray-scale image or 2D point set, stored in std::vector or [Mat](https://docs.opencv.org/4.5.2/d3/d63/classcv_1_1Mat.html).\n",
    "\n",
    "### <font color=\"green\">OpenCV Documentation</font>\n",
    "\n",
    "[**`boundingRect()`**](https://docs.opencv.org/4.5.2/d3/dc0/group__imgproc__shape.html#ga103fcbda2f540f3ef1c042d6a9b35ac7) <br>\n",
    "\n",
    "<hr style=\"border:none; height: 4px; background-color:#D3D3D3\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q8Jr80Mveq5M"
   },
   "source": [
    "# 4. Create Video Capture and Video Writer Objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Uy71R2Oeq5N"
   },
   "outputs": [],
   "source": [
    "input_video = 'motion_test.mp4'\n",
    "video_cap = cv2.VideoCapture(input_video)\n",
    "if not video_cap.isOpened():\n",
    "    print('Unable to open: ' + input_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U0qzk0aReq5N"
   },
   "outputs": [],
   "source": [
    "frame_w = int(video_cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "frame_h = int(video_cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "fps = int(video_cap.get(cv2.CAP_PROP_FPS))\n",
    "\n",
    "size = (frame_w, frame_h)\n",
    "# Original height and width after concatenation.\n",
    "size_quad = (int(2*frame_w), int(2*frame_h))\n",
    "# Resized height and width after concatenation.\n",
    "# size_quad = (int(frame_w), int(frame_h))\n",
    "\n",
    "video_out_quad = cv2.VideoWriter('video_out_quad.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, size_quad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWgkU-uteq5N"
   },
   "source": [
    "# 5. Execution and Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "22XdIcK_eq5N"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Convenience function for annotating video frames</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IygJ84VUeq5N"
   },
   "outputs": [],
   "source": [
    "def drawBannerText(frame, text, banner_height_percent = 0.08, font_scale = 0.8, text_color = (0, 255, 0), \n",
    "                   font_thickness = 2):\n",
    "    # Draw a black filled banner across the top of the image frame.\n",
    "    # percent: set the banner height as a percentage of the frame height.\n",
    "    banner_height = int(banner_height_percent * frame.shape[0])\n",
    "    cv2.rectangle(frame, (0, 0), (frame.shape[1], banner_height), (0, 0, 0), thickness = -1)\n",
    "\n",
    "    # Draw text on banner.\n",
    "    left_offset = 20\n",
    "    location = (left_offset, int(10 + (banner_height_percent * frame.shape[0]) / 2))\n",
    "    cv2.putText(frame, text, location, cv2.FONT_HERSHEY_SIMPLEX, font_scale, text_color, \n",
    "                font_thickness, cv2.LINE_AA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWHiT56Peq5O"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Create background subtraction object</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2VWKCS8ieq5O"
   },
   "outputs": [],
   "source": [
    "bg_sub = cv2.createBackgroundSubtractorKNN(history = 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T2W-z5rjeq5O"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Process video</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cF1Y-6Nheq5O"
   },
   "outputs": [],
   "source": [
    "ksize = (5, 5)\n",
    "red = (0, 0, 255)\n",
    "yellow = (0, 255, 255)\n",
    "\n",
    "# Quad view that will be built.\n",
    "#------------------------------------\n",
    "# frame_fg_mask       :  frame\n",
    "# frame_fg_mask_erode :  frame_erode\n",
    "#------------------------------------\n",
    "\n",
    "while True:\n",
    "    ret, frame = video_cap.read()\n",
    "\n",
    "    if frame is None:\n",
    "        break\n",
    "    else:\n",
    "        frame_erode = frame.copy()\n",
    "\n",
    "    # Stage 1: Motion area based on foreground mask.\n",
    "    fg_mask = bg_sub.apply(frame)\n",
    "    motion_area = cv2.findNonZero(fg_mask)\n",
    "    x, y, w, h = cv2.boundingRect(motion_area)\n",
    "\n",
    "    # Stage 2: Motion area based on foreground mask (with erosion)\n",
    "    fg_mask_erode = cv2.erode(fg_mask, np.ones(ksize, np.uint8))\n",
    "    motion_area_erode = cv2.findNonZero(fg_mask_erode)\n",
    "    xe, ye, we, he = cv2.boundingRect(motion_area_erode)\n",
    "\n",
    "    # Draw bounding box for motion area based on foreground mask\n",
    "    if motion_area is not None:\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), red, thickness = 6)\n",
    "\n",
    "    # Draw bounding box for motion area based on foreground mask (with erosion)\n",
    "    if motion_area_erode is not None:\n",
    "        cv2.rectangle(frame_erode, (xe, ye), (xe + we, ye + he), red, thickness = 6)\n",
    "\n",
    "    # Convert foreground masks to color so we can build a composite video with color annotations.\n",
    "    frame_fg_mask = cv2.cvtColor(fg_mask, cv2.COLOR_GRAY2BGR)\n",
    "    frame_fg_mask_erode= cv2.cvtColor(fg_mask_erode, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    # Annotate each video frame.\n",
    "    drawBannerText(frame_fg_mask, 'Foreground Mask')\n",
    "    drawBannerText(frame_fg_mask_erode, 'Foreground Mask Eroded')\n",
    "\n",
    "    # Build quad view.\n",
    "    frame_top = np.hstack([frame_fg_mask, frame])\n",
    "    frame_bot = np.hstack([frame_fg_mask_erode, frame_erode])\n",
    "    frame_composite = np.vstack([frame_top, frame_bot])\n",
    "\n",
    "    # Create composite video with intermediate results (quad grid).\n",
    "    fc_h, fc_w, _= frame_composite.shape\n",
    "    cv2.line(frame_composite, (0, int(fc_h/2)), (fc_w, int(fc_h/2)), yellow, thickness=1, lineType=cv2.LINE_AA)\n",
    "\n",
    "    # Resize if moviepy is unable to load the frames later.\n",
    "    # frame_composite = cv2.resize(frame_composite, None, fx=0.5, fy=0.5)\n",
    "\n",
    "    # Write video files.\n",
    "    video_out_quad.write(frame_composite)\n",
    "\n",
    "video_cap.release()\n",
    "video_out_quad.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SfnEVM9Keq5O",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "input_video = './video_out_quad.mp4'\n",
    "\n",
    "# loading output video \n",
    "clip = VideoFileClip(input_video)\n",
    "clip.ipython_display(width=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wp3Qsazweq5P"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\"> Case 1:bg_sub = cv2.createBackgroundSubtractorKNN (history = 200)</font>\n",
    "\n",
    "Notice in the top frame the book and hand are still in motion. The bounding rectangle encompasses both as well as the shadows that remain after erosion. In the bottom frame (after the book has been placed), only the hand is in motion. The shadows that persist from the book placement are learned by the model as part of the background and are no longer identified as motion associated with the foreground.\n",
    "\n",
    "![Feature-image-03c](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Feature-Image-03c.jpg)\n",
    "![Feature-image-03d](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Feature-Image-03d.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pZdEE7B9eq5P"
   },
   "source": [
    "### <font style=\"color:rgb(50,120,230)\">Case 2:bg_sub = cv2.createBackgroundSubtractorKNN (history = 400)</font>\n",
    "\n",
    "In the frame below the background subtraction model was generated with a history of 400 frames (vs. 200 above). So the model has a longer memory for what teh background scene looked like. Therefore, even though the book is no longer moving it still assesses the book as a forground object.\n",
    "\n",
    "![Feature-image-03e](https://opencv.org/wp-content/uploads/2021/08/c0-m4-Feature-Image-03e.jpg)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "04_03_Motion_Detection.ipynb",
   "provenance": []
  },
  "interpreter": {
   "hash": "1e654b3bc3aace0335b326231d51e90ebd214a7f2d0629a648660f7deb4b3382"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
